{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "17_faces_regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJE2B4XUDNWu"
      },
      "source": [
        "# Age Regression with Tensorflow Probability\n",
        "\n",
        "In this notebook you will learn how work with TFP. You will set up regression models that are able to output a gaussian conditional probability distribution. You will define different models with Keras and the Tensorflow probability framework and optimize the negative log likelihood (NLL). You will model the conditional probability distribution as a Normal distribution with a constant and flexible standart deviation $\\sigma$. The mean $\\mu$ of the CPD will depend non-linearly on the input. You will compare the NLL of the two models with the constant and felxible standart deviation $\\sigma$. As input data you will use images of faces and you will try to predict the conditional probability distribution of their age.\n",
        "\n",
        "**Dataset:** \n",
        "You work with a the UTKFace dataset. It is a large dataset with long age span (range from 0 to 116 years old). The dataset consists of over 20,000 face images with annotations of age, gender, and ethnicity. The data is already preprcessed and rescaled (80x80 pixels) so you can work with it. You will only use the information of the age.\n",
        "\n",
        "**Content:**\n",
        "* Load and and split the dataset \n",
        "* Fit a model with keras and TFP that models the CPD with a non-linear mean $\\mu$ and a constant standart deviation $\\sigma$ .\n",
        "* Fit a model with keras and TFP that models the CPD with a non-linear mean $\\mu$ and a flexible standart deviation $\\sigma$ with TFP.\n",
        "* Compare the two models based on the NLL loss on the test dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9mojpXQ4Rj-"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "# !pip install tensorflow==2.1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4IJDOa3Qkni"
      },
      "source": [
        "# !pip install tensorflow_probability==0.8.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fIXm9v3JSsK"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uxwN1Z8uZB3"
      },
      "source": [
        "import numpy as np\n",
        "import urllib\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow as tf\n",
        "%matplotlib inline\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Convolution2D, MaxPooling2D, Flatten , Activation, Dropout, Input, Concatenate\n",
        "from tensorflow.keras.utils import to_categorical \n",
        "from tensorflow.keras import optimizers\n",
        "tfd = tfp.distributions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lamum9E-dcwz"
      },
      "source": [
        "print(\"TFP Version\", tfp.__version__)\n",
        "print(\"TF  Version\",tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdnkSVWmJUXX"
      },
      "source": [
        "#### Loading the data, if it is not loaded"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxbUKL78JVKy"
      },
      "source": [
        "if not os.path.isfile('X_faces.npy'):\n",
        "    urllib.request.urlretrieve(\n",
        "    \"https://www.dropbox.com/s/5m7nmebpjysqtus/X_faces.npy?dl=1\",\n",
        "    \"X_faces.npy\")\n",
        "\n",
        "if not os.path.isfile('Y_age.npy'):\n",
        "    urllib.request.urlretrieve(\n",
        "    \"https://www.dropbox.com/s/flpyvgdqoatdw0g/Y_age.npy?dl=1\",\n",
        "    \"Y_age.npy\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B_EJ00sKIiN"
      },
      "source": [
        "X=np.load(\"X_faces.npy\")\n",
        "Y=np.load(\"Y_age.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4PHsH3dJdT6"
      },
      "source": [
        "#### Splitting the data into train, val and test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mk4yqoyEZ80S"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=201)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.33, random_state=34)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6m1YVhpZ-wu"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOc3Er5yNCUH"
      },
      "source": [
        "#### Looking at the image data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5pSRxfiNBT_"
      },
      "source": [
        "plt.figure(figsize=(20,20))\n",
        "for i in range(0,25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.imshow(X_train[i])\n",
        "    plt.title(\"Age : \"+ str(y_train[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wET5EAj1KGUk"
      },
      "source": [
        "#### Normalize the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LtYwRKyaWUR"
      },
      "source": [
        "X_train=X_train/255\n",
        "X_val=X_val/255\n",
        "X_test=X_test/255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2_-fUp88Zxn"
      },
      "source": [
        "X_train = np.array(X_train,dtype=\"float32\")\n",
        "X_val = np.array(X_val,dtype=\"float32\")\n",
        "X_test = np.array(X_test,dtype=\"float32\")\n",
        "\n",
        "y_train = np.array(y_train,dtype=\"float32\")\n",
        "y_val = np.array(y_val,dtype=\"float32\")\n",
        "y_test = np.array(y_test,dtype=\"float32\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwwWDz3ILy2V"
      },
      "source": [
        "#### Looking at the distribution of the target variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHJvwt2aKNeC"
      },
      "source": [
        "plt.figure(figsize=(14,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.hist(y_train,bins=30)\n",
        "plt.title(\"Age dist train\")\n",
        "plt.subplot(1,2,2)\n",
        "plt.hist(y_val,bins=30)\n",
        "plt.title(\"Age dist val\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WP5PYd0Mu5K"
      },
      "source": [
        "## Fit a regression model with constant variance\n",
        "In the next cells you will define and fit a model on the face images. You will use a CNN to model the mu parameter of  a gaussian conditional probability distribution, the sigma will be constant for all inputs. For the loss we use the NLL. Note that we will use the trick with a second input that will be all ones, to model the constant sigma."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fiYRgoDOxfi"
      },
      "source": [
        "kernel_size = (3, 3)\n",
        "pool_size = (2, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN7aTf9CUDEQ"
      },
      "source": [
        "def NLL(y, distr):\n",
        "  return -distr.log_prob(y) \n",
        "\n",
        "def my_dist(params): \n",
        "  return tfd.Normal(loc=params[:,0:1], scale=1e-3 + tf.math.softplus(0.05 * params[:,1:2]))# both parameters are learnable\n",
        "\n",
        "input1 = Input(shape=(80,80,3))\n",
        "input2 = Input(shape=(1,))\n",
        "x = Convolution2D(16,kernel_size,padding='same',activation=\"relu\")(input1)\n",
        "x = Convolution2D(16,kernel_size,padding='same',activation=\"relu\")(x)\n",
        "x = MaxPooling2D(pool_size=pool_size)(x)\n",
        "\n",
        "x = Convolution2D(32,kernel_size,padding='same',activation=\"relu\")(x)\n",
        "x = Convolution2D(32,kernel_size,padding='same',activation=\"relu\")(x)\n",
        "x = MaxPooling2D(pool_size=pool_size)(x)\n",
        "\n",
        "x = Convolution2D(32,kernel_size,padding='same',activation=\"relu\")(x)\n",
        "x = Convolution2D(32,kernel_size,padding='same',activation=\"relu\")(x)\n",
        "x = MaxPooling2D(pool_size=pool_size)(x)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(500,activation=\"relu\")(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(50,activation=\"relu\")(x)\n",
        "x = Dropout(0.3)(x)\n",
        "out1 = Dense(1)(x)\n",
        "out2 = Dense(1)(input2) \n",
        "params = Concatenate()([out1,out2]) \n",
        "dist = tfp.layers.DistributionLambda(my_dist)(params) #\n",
        "\n",
        "model_const_sd = Model(inputs=[input1,input2], outputs=dist) ## use a trick with two inputs, input2 is just ones\n",
        "model_const_sd.compile(tf.keras.optimizers.Adam(), loss=NLL) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dR8bdvmcZ_FA"
      },
      "source": [
        "# train the model\n",
        "history=model_const_sd.fit([X_train, np.expand_dims(np.ones(len(X_train)),1)], y_train, \n",
        "                    batch_size=16, \n",
        "                    epochs=40,\n",
        "                    verbose=1, \n",
        "                    validation_data=([X_val,np.expand_dims(np.ones(len(X_val)),1)], y_val)\n",
        "                  )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-57cWITwjGHQ"
      },
      "source": [
        "### Old API\n",
        "# model_const_sd_mean = Model(inputs=[input1,input2], outputs=dist.mean())\n",
        "# model_const_sd_sd = Model(inputs=[input1,input2], outputs=dist.stddev())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EW5SvN-0wqdz"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdN8Zqd_OR1G"
      },
      "source": [
        "#### Look at the predicted mean of the CPD on the testset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex7I5qoIZ_Cn"
      },
      "source": [
        "plt.figure(figsize=(20,20))\n",
        "for i in range(0,25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.imshow(X_test[i])\n",
        "    plt.title(\"pred : \"+ \n",
        "              str(round(float(model_const_sd([X_test[i:i+1], \n",
        "                                              np.expand_dims(np.ones(len(X_test[i:i+1])),1)]).mean()[0][0]), 2)) + \n",
        "              \"   true : \"+ str(y_test[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DYfIyPcOfCi"
      },
      "source": [
        "#### Look at the predicted mean and the predicted sigma of the CPD on the testset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYnfRtAMjZg3"
      },
      "source": [
        "for i in range(0,5):\n",
        "  plt.figure(figsize=(12,6))\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.imshow(X_test[i])\n",
        "  ### Old API\n",
        "  # plt.title(\"pred : \"+ np.str(model_const_sd_mean.predict([X_test[i:i+1],np.expand_dims(np.ones(len(X_test[i:i+1])),1)])[0][0]) + \"   true : \"+ np.str(y_test[i]))\n",
        "  # print(model_const_sd_mean.predict([X_test[i:i+1],np.expand_dims(np.ones(len(X_test[i:i+1])),1)]))\n",
        "  # print(model_const_sd_sd.predict([X_test[i:i+1],np.expand_dims(np.ones(len(X_test[i:i+1])),1)]))\n",
        "  # d = tfd.Normal(loc=model_const_sd_mean.predict([X_test[i:i+1],np.expand_dims(np.ones(len(X_test[i:i+1])),1)]), scale=model_const_sd_sd.predict([X_test[i:i+1],np.expand_dims(np.ones(len(X_test[i:i+1])),1)]))           #A\n",
        "  mu = model_const_sd([X_test[i:i+1],np.expand_dims(np.ones(len(X_test[i:i+1])),1)]).mean()\n",
        "  sigma = model_const_sd([X_test[i:i+1],np.expand_dims(np.ones(len(X_test[i:i+1])),1)]).stddev()\n",
        "  print(mu)\n",
        "  print(sigma)\n",
        "  plt.title(\"pred : \"+ str(round(float(mu), 2)) + \n",
        "            \"   true : \"+ str(y_test[i]))\n",
        "  d = tfd.Normal(loc=mu, scale=sigma)           #A\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(np.arange(-10,100,0.2),d.prob(np.arange(-10,100,0.2))[0])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzkJVe4oOkIh"
      },
      "source": [
        "## Fit a regression model with felxible variance\n",
        "In the next cells you will afain define and fit a model on the face images. You will use a CNN to model the mu parameter of a gaussian conditional probability distribution, but this time the sigma will not be constant for all inputs. Every iamge will be able to have a different sigma. For the loss we use the NLL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8vYpToHZ-7i"
      },
      "source": [
        "def NLL(y, distr):\n",
        "  return -distr.log_prob(y) \n",
        "\n",
        "def my_dist(params): \n",
        "  return tfd.Normal(loc=params[:,0:1], scale=1e-3 + tf.math.softplus(0.05 * params[:,1:2]))# both parameters are learnable\n",
        "\n",
        "inputs = Input(shape=(80,80,3))\n",
        "x = Convolution2D(16,kernel_size,padding='same',activation=\"relu\")(inputs)\n",
        "x = Convolution2D(16,kernel_size,padding='same',activation=\"relu\")(x)\n",
        "x = MaxPooling2D(pool_size=pool_size)(x)\n",
        "\n",
        "x = Convolution2D(32,kernel_size,padding='same',activation=\"relu\")(x)\n",
        "x = Convolution2D(32,kernel_size,padding='same',activation=\"relu\")(x)\n",
        "x = MaxPooling2D(pool_size=pool_size)(x)\n",
        "\n",
        "x = Convolution2D(32,kernel_size,padding='same',activation=\"relu\")(x)\n",
        "x = Convolution2D(32,kernel_size,padding='same',activation=\"relu\")(x)\n",
        "x = MaxPooling2D(pool_size=pool_size)(x)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(500,activation=\"relu\")(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(50,activation=\"relu\")(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(2)(x)\n",
        "dist = tfp.layers.DistributionLambda(my_dist)(x) \n",
        "\n",
        "model_flex = Model(inputs=inputs, outputs=dist)\n",
        "model_flex.compile(tf.keras.optimizers.Adam(), loss=NLL) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ddl6NgxZ-0q"
      },
      "source": [
        "# train the model\n",
        "history=model_flex.fit(X_train, np.array(y_train,dtype=\"float32\"), \n",
        "                  batch_size=16, \n",
        "                  epochs=40,\n",
        "                  verbose=1, \n",
        "                  validation_data=(X_val, np.array(y_val,dtype=\"float32\")))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wx5xNhDx7O07"
      },
      "source": [
        "### Old API\n",
        "# model_mean = Model(inputs=inputs, outputs=dist.mean())\n",
        "# model_sd = Model(inputs=inputs, outputs=dist.stddev())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DItfSKvh1Whm"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['loss', 'val_loss'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKdLA-HgPmmL"
      },
      "source": [
        "#### Look at the predicted mean of the CPD on the testset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0e8El8k1rLb"
      },
      "source": [
        "plt.figure(figsize=(20,20))\n",
        "for i in range(0,25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.imshow(X_test[i])\n",
        "    plt.title(\"pred : \"+ str(round(float(model_flex(X_test[i:i+1]).mean()[0][0]),2)) + \n",
        "              \"   true : \"+ str(y_test[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deXn_qkfPtgW"
      },
      "source": [
        "#### Look at the predicted mean and the predicted sigma of the CPD on the testset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci17D_-o3JUs"
      },
      "source": [
        "for i in range(0,5):\n",
        "  plt.figure(figsize=(12,6))\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.imshow(X_test[i])\n",
        "  ### Old API\n",
        "  # plt.title(\"pred : \"+ np.str(model_mean.predict(X_test[i:i+1])[0][0]) + \"   true : \"+ np.str(y_test[i]))\n",
        "  # print(model_mean.predict(X_test[i:i+1]))\n",
        "  # print(model_sd.predict(X_test[i:i+1]))\n",
        "  # d = tfd.Normal(loc=model_mean.predict(X_test[i:i+1]), scale=model_sd.predict(X_test[i:i+1]))           #A\n",
        "  mu = model_flex(X_test[i:i+1]).mean()\n",
        "  sigma = model_flex(X_test[i:i+1]).stddev()\n",
        "  print(mu)\n",
        "  print(sigma)\n",
        "  plt.title(\"pred : \"+ str(round(float(mu),2)) + \"   true : \"+ str(y_test[i]))\n",
        "  d = tfd.Normal(loc=mu, scale=sigma)           #A\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(np.arange(-10,100,0.2),d.prob(np.arange(-10,100,0.2))[0])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvkp3IjxPz_8"
      },
      "source": [
        "#### Exercise\n",
        "Calculate the MSE the RMSE and the NLL for both models on the testset.  \n",
        "Which model would you prefer in practice and why?  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpkbW0UpsQXt"
      },
      "source": [
        "### Your code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}